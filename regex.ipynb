{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551fdc9-dee5-4b3b-8b61-ae084001bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf26fa8-a2ac-4c20-8625-075be9602a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_statistical_method(pages_text, method_keywords):\n",
    "    \"\"\"在文本中查找某种统计方法并记录其位置，返回包含该方法的前后各一句，按句子分割\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # 构建正则表达式，忽略大小写并允许匹配单复数形式\n",
    "    patterns = {\n",
    "        keyword: re.compile(rf\"\\b{re.escape(keyword)}(?:[\\s\\-_])?s?\\b\", re.IGNORECASE)\n",
    "        for keyword in method_keywords\n",
    "    }\n",
    "\n",
    "    for page_num, page_text in enumerate(pages_text, start=1):\n",
    "        # 根据句号、感叹号、问号等分割文本为句子\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', page_text)\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # 对每个正则表达式进行查找匹配项\n",
    "            for keyword, pattern in patterns.items():\n",
    "                if re.search(pattern, sentence):\n",
    "                    # 获取前后两句\n",
    "                    prev_sentence = sentences[i-1].strip() if i > 0 else \"\"\n",
    "                    next_sentence = sentences[i+1].strip() if i < len(sentences) - 1 else \"\"\n",
    "\n",
    "                    # 记录结果\n",
    "                    result = {\n",
    "                        \"page\": page_num,\n",
    "                        \"method\": keyword,\n",
    "                        \"sentence\": sentence.strip(),\n",
    "                        \"prev_sentence\": prev_sentence,\n",
    "                        \"next_sentence\": next_sentence\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92057c-17f9-48bd-bacf-17e7909dac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively extract all strings from a nested YAML structure\n",
    "def extract_all_strings(yaml_data):\n",
    "    strings = []\n",
    "    \n",
    "    # Recursive function to walk through the YAML structure\n",
    "    def recursive_extract(value):\n",
    "        if isinstance(value, str):\n",
    "            # Separate main terms from parentheses\n",
    "            main_term = re.split(r'\\s*\\(.*?\\)', value)[0].strip()  # Extract main term before parentheses\n",
    "            if main_term:\n",
    "                strings.append(main_term)\n",
    "\n",
    "            # Extract content inside parentheses separately\n",
    "            parentheses_content = re.findall(r'\\((.*?)\\)', value)\n",
    "            for content in parentheses_content:\n",
    "                terms_in_parentheses = content.split(',')  # Split by commas inside parentheses\n",
    "                strings.extend([term.strip() for term in terms_in_parentheses])  # Add each term separately\n",
    "                \n",
    "        elif isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                recursive_extract(k)  # Keys might also be strings\n",
    "                recursive_extract(v)  # Recursively process values\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                recursive_extract(item)  # Process each item in the list\n",
    "\n",
    "    recursive_extract(yaml_data)\n",
    "    return strings\n",
    "\n",
    "# Reading the YAML file with UTF-8 encoding\n",
    "with open('ontology.yaml', 'r', encoding='utf-8') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "\n",
    "# Extract all strings\n",
    "all_strings = extract_all_strings(yaml_content)\n",
    "\n",
    "# Print the extracted strings\n",
    "print(all_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efb314-6be7-440a-9b8c-ef63b0f12663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_result(pdf_path, method_keywords):\n",
    "    pages_text = extract_text_from_pdf(pdf_path)\n",
    "    results = find_statistical_method(pages_text, method_keywords)\n",
    "    \n",
    "    if results:\n",
    "        return results\n",
    "    else:\n",
    "        return \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afa9b1-bb90-4cce-b953-3315a7234848",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"neuro-1.pdf\"  # 替换为你的PDF文件路径\n",
    "method_keywords = all_strings  # 替换为你要搜索的统计方法关键词\n",
    "result = find_result(pdf_path, method_keywords)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
